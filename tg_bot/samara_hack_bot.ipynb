{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "samara_hack_bot.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBp7CXWm9DnE",
        "outputId": "6382a6fc-79aa-435f-dac6-34f78edfe815"
      },
      "source": [
        "# !pip3 install pytelegrambotapi --upgrade\n",
        "# !pip3 install urllib3==1.25.4\n",
        "# !git clone https://github.com/roboflow-ai/yolov5\n",
        "# !pip install -U -r yolov5/requirements.txt\n",
        "# !mkdir /content/test_img\n",
        "# !mkdir /content/test_img_pred\n",
        "%cd yolov5\n",
        "# # !pip install torch==1.7.0+cu101 torchvision==0.6.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkYF-KT_9Dky"
      },
      "source": [
        "import telebot\n",
        "from token import TOKEN\n",
        "\n",
        "token = TOKEN\n",
        "bot = telebot.TeleBot(token)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pPXEi5ZUbfB"
      },
      "source": [
        "import argparse\n",
        "import time\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "from numpy import random\n",
        "\n",
        "from models.experimental import attempt_load\n",
        "from utils.datasets import LoadStreams, LoadImages\n",
        "from utils.general import check_img_size, non_max_suppression, apply_classifier, scale_coords, xyxy2xywh, \\\n",
        "    strip_optimizer, set_logging, increment_path\n",
        "from utils.plots import plot_one_box\n",
        "from utils.torch_utils import select_device, load_classifier, time_synchronized\n",
        "\n",
        "def detect(save_img=False):\n",
        "    weights = 'yolov5s.pt'\n",
        "    source = 'test_img'\n",
        "    save_txt = 'test_img'\n",
        "    project = 'test_img_pred'\n",
        "    view_img = False\n",
        "    img_size = 640\n",
        "    conf_thres = 0.25\n",
        "    iou_thres = 0.45\n",
        "    device = ''\n",
        "    name = 'exp'\n",
        "    exist_ok = True\n",
        "    agnostic_nms=True\n",
        "    classes = 0\n",
        "    save_conf = False\n",
        "    augment = True\n",
        "    source, weights, view_img, save_txt, imgsz = source, weights, view_img, save_txt, img_size\n",
        "    webcam = source.isnumeric() or source.endswith('.txt') or source.lower().startswith(\n",
        "        ('rtsp://', 'rtmp://', 'http://'))\n",
        "\n",
        "    # Directories\n",
        "    save_dir = Path(increment_path(Path(project) / name, exist_ok=exist_ok))  # increment run\n",
        "    (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n",
        "\n",
        "    # Initialize\n",
        "    set_logging()\n",
        "    device = select_device(device)\n",
        "    half = device.type != 'cpu'  # half precision only supported on CUDA\n",
        "\n",
        "    # Load model\n",
        "    model = attempt_load(weights, map_location=device)  # load FP32 model\n",
        "    imgsz = check_img_size(imgsz, s=model.stride.max())  # check img_size\n",
        "    if half:\n",
        "        model.half()  # to FP16\n",
        "\n",
        "    # Second-stage classifier\n",
        "    classify = False\n",
        "    if classify:\n",
        "        modelc = load_classifier(name='resnet101', n=2)  # initialize\n",
        "        modelc.load_state_dict(torch.load('weights/resnet101.pt', map_location=device)['model']).to(device).eval()\n",
        "\n",
        "    # Set Dataloader\n",
        "    vid_path, vid_writer = None, None\n",
        "    if webcam:\n",
        "        view_img = True\n",
        "        cudnn.benchmark = True  # set True to speed up constant image size inference\n",
        "        dataset = LoadStreams(source, img_size=imgsz)\n",
        "    else:\n",
        "        save_img = True\n",
        "        dataset = LoadImages(source, img_size=imgsz)\n",
        "\n",
        "    # Get names and colors\n",
        "    names = model.module.names if hasattr(model, 'module') else model.names\n",
        "    colors = [[random.randint(0, 255) for _ in range(3)] for _ in names]\n",
        "\n",
        "    # Run inference\n",
        "    t0 = time.time()\n",
        "    img = torch.zeros((1, 3, imgsz, imgsz), device=device)  # init img\n",
        "    _ = model(img.half() if half else img) if device.type != 'cpu' else None  # run once\n",
        "    for path, img, im0s, vid_cap in dataset:\n",
        "        img = torch.from_numpy(img).to(device)\n",
        "        img = img.half() if half else img.float()  # uint8 to fp16/32\n",
        "        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
        "        if img.ndimension() == 3:\n",
        "            img = img.unsqueeze(0)\n",
        "\n",
        "        # Inference\n",
        "        t1 = time_synchronized()\n",
        "        pred = model(img, augment=augment)[0]\n",
        "\n",
        "        # Apply NMS\n",
        "        pred = non_max_suppression(pred, conf_thres, iou_thres, classes=classes, agnostic=agnostic_nms)\n",
        "        t2 = time_synchronized()\n",
        "\n",
        "        # Apply Classifier\n",
        "        if classify:\n",
        "            pred = apply_classifier(pred, modelc, img, im0s)\n",
        "\n",
        "        # Process detections\n",
        "        for i, det in enumerate(pred):  # detections per image\n",
        "            if webcam:  # batch_size >= 1\n",
        "                p, s, im0 = Path(path[i]), '%g: ' % i, im0s[i].copy()\n",
        "            else:\n",
        "                p, s, im0 = Path(path), '', im0s\n",
        "\n",
        "            save_path = str(save_dir / p.name)\n",
        "            txt_path = str(save_dir / 'labels' / p.stem) + ('_%g' % dataset.frame if dataset.mode == 'video' else '')\n",
        "            s += '%gx%g ' % img.shape[2:]  # print string\n",
        "            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
        "            if len(det):\n",
        "                # Rescale boxes from img_size to im0 size\n",
        "                det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n",
        "\n",
        "                # Print results\n",
        "                for c in det[:, -1].unique():\n",
        "                    n = (det[:, -1] == c).sum()  # detections per class\n",
        "                    s += '%g %ss, ' % (n, names[int(c)])  # add to string\n",
        "\n",
        "                # Write results\n",
        "                for *xyxy, conf, cls in reversed(det):\n",
        "                    if save_txt:  # Write to file\n",
        "                        xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
        "                        line = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # label format\n",
        "                        with open(txt_path + '.txt', 'a') as f:\n",
        "                            f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n",
        "\n",
        "                    if save_img or view_img:  # Add bbox to image\n",
        "                        label = '%s %.2f' % (names[int(cls)], conf)\n",
        "                        plot_one_box(xyxy, im0, label=label, color=colors[int(cls)], line_thickness=3)\n",
        "\n",
        "            # Print time (inference + NMS)\n",
        "            print('%sDone. (%.3fs)' % (s, t2 - t1))\n",
        "\n",
        "            # Stream results\n",
        "            if view_img:\n",
        "                cv2.imshow(str(p), im0)\n",
        "                if cv2.waitKey(1) == ord('q'):  # q to quit\n",
        "                    raise StopIteration\n",
        "\n",
        "            # Save results (image with detections)\n",
        "            if save_img:\n",
        "                if dataset.mode == 'images':\n",
        "                    cv2.imwrite(save_path, im0)\n",
        "                else:\n",
        "                    if vid_path != save_path:  # new video\n",
        "                        vid_path = save_path\n",
        "                        if isinstance(vid_writer, cv2.VideoWriter):\n",
        "                            vid_writer.release()  # release previous video writer\n",
        "\n",
        "                        fourcc = 'mp4v'  # output video codec\n",
        "                        fps = vid_cap.get(cv2.CAP_PROP_FPS)\n",
        "                        w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "                        h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "                        vid_writer = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*fourcc), fps, (w, h))\n",
        "                    vid_writer.write(im0)\n",
        "\n",
        "    if save_txt or save_img:\n",
        "        s = f\"\\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}\" if save_txt else ''\n",
        "        print(f\"Results saved to {save_dir}{s}\")\n",
        "\n",
        "    print('Done. (%.3fs)' % (time.time() - t0))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     parser = argparse.ArgumentParser()\n",
        "#     parser.add_argument('--weights', nargs='+', type=str, default='yolov5s.pt', help='model.pt path(s)')\n",
        "#     parser.add_argument('--source', type=str, default='data/images', help='source')  # file/folder, 0 for webcam\n",
        "#     parser.add_argument('--img-size', type=int, default=640, help='inference size (pixels)')\n",
        "#     parser.add_argument('--conf-thres', type=float, default=0.25, help='object confidence threshold')\n",
        "#     parser.add_argument('--iou-thres', type=float, default=0.45, help='IOU threshold for NMS')\n",
        "#     parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n",
        "#     parser.add_argument('--view-img', action='store_true', help='display results')\n",
        "#     parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')\n",
        "#     parser.add_argument('--save-conf', action='store_true', help='save confidences in --save-txt labels')\n",
        "#     parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --class 0, or --class 0 2 3')\n",
        "#     parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')\n",
        "#     parser.add_argument('--augment', action='store_true', help='augmented inference')\n",
        "#     parser.add_argument('--update', action='store_true', help='update all models')\n",
        "#     parser.add_argument('--project', default='runs/detect', help='save results to project/name')\n",
        "#     parser.add_argument('--name', default='exp', help='save results to project/name')\n",
        "#     parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')\n",
        "#     opt = parser.parse_args()\n",
        "#     print(opt)\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         if opt.update:  # update all models (to fix SourceChangeWarning)\n",
        "#             for opt.weights in ['yolov5s.pt', 'yolov5m.pt', 'yolov5l.pt', 'yolov5x.pt']:\n",
        "#                 detect()\n",
        "#                 strip_optimizer(opt.weights)\n",
        "#         else:\n",
        "#             detect()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctwqqNx29Dij"
      },
      "source": [
        "@bot.message_handler(content_types=[\"photo\"])\n",
        "def detect_send(message):\n",
        "    raw = message.photo[2].file_id\n",
        "    name = 'test_img/'+raw+\".jpg\"\n",
        "    file_info = bot.get_file(raw)\n",
        "    downloaded_file = bot.download_file(file_info.file_path)\n",
        "    with open(name,'wb') as new_file:\n",
        "        new_file.write(downloaded_file)\n",
        "    detect()\n",
        "    img = open('test_img_pred/exp/'+raw+\".jpg\", 'rb')\n",
        "    bot.send_message(message.chat.id, \"Запрос от\\n*{name} {last}*\".format(name=message.chat.first_name, last=message.chat.last_name), parse_mode=\"Markdown\") #от кого идет сообщение и его содержание\n",
        "    bot.send_photo(message.chat.id, img)\n",
        "    bot.send_message(message.chat.id, \"*{name}!*\\n\\nСпасибо за инфу\".format(name=message.chat.first_name, last=message.chat.last_name, text=message.text), parse_mode=\"Markdown\") #то что пойдет юзеру после отправки сообщения\n",
        "    os.system('rm -f test_img_pred/exp/labels/*')\n",
        "    os.system('rm -f test_img_pred/exp/*')\n",
        "    os.system('rm -f test_img/*')"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOT9qBmpZnLe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b984cd4-ff00-4e93-e43e-f853302eec90"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    bot.polling(none_stop=True)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using torch 1.10.0+cu111 CPU\n",
            "\n",
            "Model Summary: 232 layers, 7459581 parameters, 0 gradients\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fusing layers... \n",
            "image 1/1 /content/test_img/AgACAgIAAxkBAAMZYaDhWFzxWEFRvwPjkvF0SzAOT7kAAje5MRvCtAhJiUL_3hBUDdABAAMCAAN4AAMiBA.jpg: 448x640 11 persons, 2 chairs, 1 couchs, 1 potted plants, Done. (0.572s)\n",
            "Results saved to /content/test_img_pred/exp\n",
            "1 labels saved to /content/test_img_pred/exp/labels\n",
            "Done. (0.597s)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using torch 1.10.0+cu111 CPU\n",
            "\n",
            "Model Summary: 232 layers, 7459581 parameters, 0 gradients\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fusing layers... \n",
            "image 1/1 /content/test_img/AgACAgIAAxkBAAMhYaDntm4FnRzhLgT7btgv_URX4-0AAje5MRvCtAhJiUL_3hBUDdABAAMCAAN4AAMiBA.jpg: 448x640 11 persons, 2 chairs, 1 couchs, 1 potted plants, Done. (0.568s)\n",
            "Results saved to /content/test_img_pred/exp\n",
            "1 labels saved to /content/test_img_pred/exp/labels\n",
            "Done. (0.596s)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using torch 1.10.0+cu111 CPU\n",
            "\n",
            "Model Summary: 232 layers, 7459581 parameters, 0 gradients\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fusing layers... \n",
            "image 1/1 /content/test_img/AgACAgIAAxkBAAMmYaDo4LqmauuFCIpaHjWkyT86v5EAAoK2MRuf0AFJYN1XGlXZ_KABAAMCAAN4AAMiBA.jpg: 480x640 1 persons, Done. (0.649s)\n",
            "Results saved to /content/test_img_pred/exp\n",
            "1 labels saved to /content/test_img_pred/exp/labels\n",
            "Done. (0.663s)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using torch 1.10.0+cu111 CPU\n",
            "\n",
            "Model Summary: 232 layers, 7459581 parameters, 0 gradients\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fusing layers... \n",
            "image 1/1 /content/test_img/AgACAgIAAxkBAAMqYaDpBHwasEOaTHPyCZczD4-Z4qMAAoa2MRuf0AFJzWfYMQ5fQtEBAAMCAAN4AAMiBA.jpg: 480x640 1 elephants, Done. (0.605s)\n",
            "Results saved to /content/test_img_pred/exp\n",
            "1 labels saved to /content/test_img_pred/exp/labels\n",
            "Done. (0.625s)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using torch 1.10.0+cu111 CPU\n",
            "\n",
            "Model Summary: 232 layers, 7459581 parameters, 0 gradients\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fusing layers... \n",
            "image 1/1 /content/test_img/AgACAgIAAxkBAAMuYaDpeTCxFh_MEvZSo-5AITydy78AAom2MRuf0AFJ-pA8y_Ysg9QBAAMCAAN4AAMiBA.jpg: 480x640 Done. (0.604s)\n",
            "Results saved to /content/test_img_pred/exp\n",
            "0 labels saved to /content/test_img_pred/exp/labels\n",
            "Done. (0.624s)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using torch 1.10.0+cu111 CPU\n",
            "\n",
            "Model Summary: 232 layers, 7459581 parameters, 0 gradients\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fusing layers... \n",
            "image 1/1 /content/test_img/AgACAgIAAxkBAAMyYaD5VZV1cNs3YQIo1AtEdrI8AX8AAje5MRvCtAhJiUL_3hBUDdABAAMCAAN4AAMiBA.jpg: 448x640 11 persons, 2 chairs, 1 couchs, 1 potted plants, Done. (0.579s)\n",
            "Results saved to /content/test_img_pred/exp\n",
            "1 labels saved to /content/test_img_pred/exp/labels\n",
            "Done. (0.606s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q309Ex9eZUuv"
      },
      "source": [
        ""
      ],
      "execution_count": 21,
      "outputs": []
    }
  ]
}